Speichersystem
**************************************************************

Begriffe:
    - Hauptspeicher / Primärspeicher (Main memory):
        - RAM
        - Während der Ausführung befinden sich Programme und Daten hier
        - Direkt adressierbar: Speicheradresse führt direkt zu Inhalt
    - Sekundärspeicher:
        - z.B. Festplatte
        - Zur persistenten Programm- und Datenspeicherung
        - Indirekt adressierbar mit Schnittstellen-Hardware und -Software


Grundlegende Speicherprinzipien:
- In elektronischen Bauteilen
- Grundlegende Prinzipien:
    1. Direkt adressierbarer Speicher
        - Über Adresse wird direkt die gewünschte Speicherstelle angesprochen
        - Zugriff: Auf jede Stelle gleich schnell (unmittelbar)
        - Grösse pro Zelle: 1 Byte (8 bit)
        - z.B. RAM, ROM
    2. Mehrportspeicher
        - Speicher mit mehreren Zugriffspfaden
        - z.B. wenn Prozessor und Peripheriekontroller gleichzeitig auf gleiche Speicherstellen zugreifen können müssen
        - Umgesetzt mit Semaphoren
    3. Schieberegisterspeicher
        - Bitmuster wird durch Kette von ein Bit grossen Speicherzellen (z.B. Flip-Flop) verschoben (vorne rein, hinten raus)
        - Einzelne Bits sind nicht direkt adressierbar
        - Für Umwandlung von seriellen in parallele Datenströme (und umgekehrt)
        - z.B. LAN-Schnittstelle (z.B. Ethernet, Token-Ring)
    4. FIFO
        - Für Buffer (Ein- und Ausgabepuffer)
        - Verwaltungsprinzip: Lese- und Schreibpointer
        - z.B. für Schnittstelle der Tastatur
    5. Stapelspeicher (Stack)
        - LIFO Speicher
        - Bekannteste Anwendung: Stack des Rechners (für lokale Variablen, etc.)
        - Verwaltungsprinzip: Stackpointer (+ Grösse des Stackframes, die bekannt ist)
    6. Assoziativspeicher
        - Mit Teilinformation des Eintrags kann ganzer Eintrag gefunden werden
        - Für Zugriff ist keine Adresse nötig (nur die Teilinformation des Inhalts) -> Inhaltsadressierte Speicher
        - Es wird quasi eine Maske über die Einträge gelegt und nur der Bereich betrachtet, den die Maske freigibt
        - z.B. Datenbank: Primärschlüssel ist Adressierungselement (und gleichzeitig Inhalt)


Speicherhierarchie & Lokalitätsprinzip:
- Anforderungen an Speicher:
    - minimale Zugriffszeit
    - minimale Kosten pro gespeichertes Bit
    -> Widerspruch: Deshalb Speicherhierarchie
- Arten:
    - CPU-interne Register: Schnellster und teuerster Speicher (nur in sehr kleiner Anzahl vorhanden)
    - Pufferspeicher (cache memory): Direkt an Prozessor angekoppelt (nicht sichtbar für Software)
    - Hauptspeicher: DRAM, SRAM
    - Massenspeicher: Billigster und langsamster Speicher
- Ziel: Mit möglichst geringen Kosten im Mittel möglichst an die Zugriffszeit des schnellsten Speicher heranzukommen
- Anwenderprogramm sieht einzigen grossen Adressraum (Speicherhierarchie ist vor ihm versteckt)
- Zur Verarbeitung durch Prozessor müssen Anweisungen / Daten immer in Prozessornahe Speicher kopiert werden
- Lokalitätseffekt:
    - Der Arbeitsbereich W(t-T, t) bleibt für größere Zeiträume unverändert
    -> Lokalitätseffekt ausnutzen: Arbeitsbereich möglichst im schnellen Speicher halten
    - Räumliche Lokalität:
        - "Wenn auf Adresse X zugegriffen wird, wird wahrscheinlich als nächstes auf Nachbaradresse zugegriffen"
        - Erkenntnis: Wenn wir X in laden, auch gleich Nachbarbyte laden
    - Zeitliche Lokalität:
        - "Wenn auf Adresse X zugegriffen wird, wird wahrscheinlich schon bald nochmals auf Adresse X zugegriffen"
        - Erkenntnis: Daten, auf die gerade zugegriffen wurde, im schnellsten Speicher halten
    - Platz in den Registern ist begrenzt. Deshalb: Cache memory zwischen Registern und Hauptspeicher
- Realisierung der Speicherhierarchie:
    - Massenspeicher <--- Hauptspeicher <--- Pufferspeicher (cache) <--- (Adresstransformation) --- Register / CPU
    - Mehrprozessorsystem:
        - Eigener Hauptspeicher pro Prozess
        - Problem: Datenkohärenz (CPU A schreibt in seinen Pufferspeicher. Was wenn B diese Info liest, befor sie in den Massenspeicher gelangt?)


Cache:
- CPU ist schneller, als Hauptspeicher. Damit Hauptspeicher CPU nicht bremst: Cache memory
- Benutzer kann nicht auf cache memory zugreifen
- Cache ist assoziativer Speicher
- Cache wird auf verschiedenen Leveln implementiert (Level1-Cache, Level2-Cache, Level3-Cache), oft direkt auf dem Chip
- Hit rate: Anteil aller Speicherzugriffe, bei denen Info im Cache gefunden wird
- Formel für Zugriffszeit:
    - Teff = h ⋅ TC + (1 – h) ⋅ TM
    - Teff: Effektive mittlere Zugriffszeit
    - TC: Zugriffszeit Cache
    - TM: Zugriffszeit Hauptspeichers
    - h: Hit rate
    - Charakteristische Eigenschaften:
        - TC << TM (Zugriffszeit auf Cache ist viel kleiner als Zugriffszeit auf Hauptspeicher)
        - h >> (1-h) (die meisten Infos können aus dem Cache geladen werden)
- Funktionsweise:
    - Cache-Steuerlogik (als Hardware realisiert) speichert benötigte Daten automatisch im Cache
    - Hauptspeicher wird in Blöcke der Grösse 2^i Byte aufgeteilt (Vorteil Zweierpotenzen: Hinterste i Bit von Adresse abschneiden => Blocknr.)
    - Die Blöcke durchnummeriert (Block, der bei der Adresse 0 beginnt, ist der erste Block)
    - Cache-Speicher besteht aus 2^k Cache-Zeilen (cache line)
    - Cache-Zeile besteht aus Tag (Blocknr.), Gültigkeitsbit (valid flag) und einem einen Datenblock (der Grösse 2^i Byte)
    - Blockweise Pufferung hat den Vorteil, dass auch benachbarte Bits gleich in den Cache geladen werden
    - Bei Speicherzugriff wird geprüft, ob sich Datum bereits im Cache befindet:
        1. Blocknummer bestimmen
        2. In allen gültigen (valid flag == 1) Blocknummer vergleichen
        3. Falls gefunden: Gesuchtes Byte aus Cache-Zeile extrahieren und dem Prozessor übergeben
        4. Falls nicht gefunden: Entsprechende Cache-Zeile gesamthaft aus Hauptspeicher laden, gesuchtes Byte an Prozessor übergeben
- Caching lohnt sich erst beim zweiten Zugriff auf ein Datum (oder wenn ein benachbartes Byte schon zuvor in den Cache geladen wurde)
- Bei vollem Cache müssen nach und nach gecachete Inhalte überschreiben werden, obwohl sie noch gültig wären (invalide Zeilen werden zuerst überschrieben)
- Peripherie ist nicht cachebar
- Schwierigkeit: wenn gecachete Inhalte verändert werden, muss Cache geleert werden


Verwaltungsgrundlagen
**************************************************************

Abbildungsfunktion:
- Beim Kompilieren werden Namen (z.B. von Variablen, Methoden) in Adressen umgewandelt => Programmadressen (virtuelle / logische Adressen)
- Programmadressen != Speicheradressen (physische Adressen), sondern virtualisierte Adressen
- Warum Virtualisierung: Damit die parallel auszuführenden Programme beliebig im Hauptspeicher platziert werden können
- Von Programmadresse zu Speicheradresse: Adressumsetzung / Adresstransformation (findet im MMU-Chip statt -> Memory Management Unit)
- Heute wird pro Adresse 1 Byte adressiert (traf für historische Rechner nicht zu)


Dynamische Speicherbereitstellung:
- Dynamische Daten: Daten, deren Lebenszeit kürzer ist, als die Programmlaufzeit
- Stack: Speicher wird im Rahmen der Stack Frames automatisch alloziert
- Heap: Lebensdauer orientiert sich nicht an der Ausführung von Funktionen -> Deshalb verwalten wir dort den Speicher selber (in C, in Java machts die JVM)
- Heap-Management wird durch Programmiersprache zur Verfügung gestellt
- Fester Adressbereich wird für Heap reserviert
- Speicherblöcke auf dem Heap können variabel, fix oder stufenweise realisiert sein -> C  arbeitet mit variablen Bereichsgrössen (universell!)
- C: malloc(size_t size) / free(void *ptr) -> OS gibt uns einen (im vituellen Adressraum) zusammenhängenden Block auf dem Speicher zurück


Verwaltungsalgorithmen:
- Anforderungen an die dynamische Speicherzuordnung:
    - Flexible Zuordnungsgrösse: Länge des zu allozierenden Bereichs soll bytegenau erfüllt werden
    - Zusammenhängende Zuordnungsbereiche: Sonst funktionieren z.B. Arrays nicht mehr
    - Schnellstmögliche Zuordnung: Speicherallokation mit möglichst wenig Aufwand -> möglichst schnell verfügbar
    - Maximale Speichernutzung: Keine oder wenige nutzlose Lücken zwischen Speicherblöcken
    - Adressraumausrichtung: Einschränkende Regeln für Datentypen, die mehr als ein Byte umfassen
    -> Problem: Teilweise widersprüchliche Ziele
- Fragmentierung (Zerstückelung des freien Speichers):
    - Extern:
        - Im Speicher ist zwar noch genug Speicher vorhanden, allerdings nicht in zusammenhängender Form (Grund: Variable Blockgrössen in Heap-Verwaltung)
        - Lösung wäre Defragmentierung. Kann man hier aber nicht, da man keine Liste mit allen belegten Speicherfragmenten hat
    - Intern:
        - Wenn ein Algorithmus einheitlich grosse Zuordnungseinheiten braucht -> es entstehen "nicht nutzbare reste" (Verschnitt)


Grundprinzip der Speicherzuordnung:
- Dienste des Heap-Managements: Bereich frei wählbarer grösse allozieren / Bereich wieder freigeben
- Moderne Programmiersprachen: Implizite Freigabe
    - Referenzen auf Variable werden gezählt (Code dazu wird vom Compiler eingefügt)
    - Zu gegebener Zeit löscht Garbage Collector alle Variablen, auf die keine Referenzen mehr existieren


Implementierungsvarianten Speicherzuordnung:
- A: Variable Zuordnungsgröße
    - Verwaltungsintern werden variabel grosse Blöcke belegt und freigegeben
    - Verwaltungssoftware hat die Aufgabe, über belegte und freie Bereiche Buch zu führen
    - Implementiert mit verketteter Liste für freie Speicher (meist geordnet nach Adressen -> benachbarte Freibereiche lassen sich leichter kombinieren)
    - 2 Varianten: Freie oder besetzte Blocks speichern
    - Jeder Eintrag hat drei Einträge: Startadresse, Länge des Blocks, Pointer auf nächsten Block
    - Algorithmen für die Speicherreservierung:
        - First Fit (erster freier Block wird genommen, wenn gross genug) -> starke Fragmentierung weil sich am Anfang des Speichers kleine Blöcke sammeln
        - Next Fit (wie "First Fit", es wird aber bei der letzten Füllung gestartet) -> noch stärkere Fragmentierung (im ganzen Speicherbereich)
        - Best Fit (Kleinstmöglicher Block wird gesucht) -> Wahrscheinlich muss ganze Liste durchsucht werden (Aufwand!), aber weniger Fragmentierung
        - Worst Fit ("Best Fit" auf den Kopf gestellt) -> Will vermeiden, dass es unnützlich kleine Restlücken gibt, nur mässig leistungsfähig
        -> Keiner davon optimal, kommt auf Anwendung an
- Variante B: Feste Blockgrößen bzw. Größenklassen
    - Mehrere Listen mit Blöcken einheitlicher Grösse
    - Allokation: Angefragter Speicher wird auf die nächsthöhere feste Blockgrösse aufgerundet
    - Objekt wird immer in die kleinste Klasse gelegt, in die es passt
    - Nachteile: Rekombinieren benachbarter Bereiche ist sehr aufwendig (falls Listenausgleich) + Verwaltungsaufwand für Freilisten
    - Algorithmus für Speicherreservierung:
        - Quick Fit (getrennte Freiliste pro Blockgrösse) -> sehr schnelle Allokation, da pro Liste erster freier Block genommen werden kann
    - Grössenklassen:
        - Normalerweise 2^k Byte -> 16, 32, 64, ...
        - Variante: 3 * 2^k Byte -> 16, 24, 32, 48, ... -> weniger Verschnitt
    - Wenn keine freien Blöcke einer bestimmten Grössenklasse mehr vorhanden:
        - Vom OS neuen Speicher anfordern
        - Grössere Speicher aufteilen / kleinere kombinieren
- Variante C: Mehrfache einer festen Blockgröße
    - Speicherbereich wird in gleich grosse Blöcke aufgeteilt
    - Wenn Anforderung grösser als Blockgrösse: Mehrere Blöcke kombinieren
    - Interne Fragmentierung ist auf Blockgrösse begrenzt, allerdings kann externe Fragmentierung auftreten
    - Interne Datenhaltung:
        - Verkettete Liste
        - Bitliste
    - Suche nach freiem Block: first, next, best und worst fit
- Variante D: Buddy-System
    - Mischung aus den verschiedenen Zuordnungsarten
    - Mit gesamtem Bereich starten
    - Allokation: Gesamtbereich wird solange zweigeteilt, bis Anforderung gerade noch befriedigt werden kann (z.B. kommen 90KB in einen 128KB-Block)
    - Effektiv verwendete Speicherbereiche sind immer 2-er Potenzen
    - Auch hier entsteht Fragmentierung
    - Blockorganisation kann mit Binärbaum dargestellt werden
    - Buddies: Zwei Speicherbereiche der gleichen Grösse -> sind diese nebeneinander, kann sehr schnell auf den zweiten zugegriffen werden
    - Chance, dass ein Buddy gesucht wird ist gross (da oft 2 gleich grosse Variablen verwendet werden)
    - Speicherfreigabe: Zwei benachbarte Buddies werden zu einem grösseren Speicherbereich kombiniert
    - Freibereiche werden in Freilisten verwaltet (Eine Liste pro Bereichsgrösse)


Verwaltung von Prozessadressräumen:
- Ausführbare Datei (executable) enthält alle Informationen, die OS zum Laden und Starten des Programms benötigt
- OS verhindert, dass Programe auf den Adressraum anderer Programme zugreifen können
- Compiler gruppiert logisch in verschiedene Teile:
        - uninitialisierte Daten (int a;) -> .bss-Sektion (nur Speicher reservieren)
        - vorinitialisierte Daten (int b=15;) ->.data-Sektion (Speicher reservieren und mit Initialwerten belegen)
        - Programmcode -> .text-Sektion (Maschinencode in Speicher laden, mit Schreibschutz versehen, da sich dieser während der Ausführung nicht ändert)
- Programm besteht aus:
    - Kennung (magische Zahl)
    - Programmstartadresse
    - Sektionsbeschreibungen (Anahl, Startadressen, Grössen, Typen)
    - .text (Maschinencode)
    - .data (Initialisierungwerte der Variablen)
    - Adressraum des Programms wird aufgeteilt in:
        - Text-Region (Maschinencode des Programms)
        - Data-Region (Initialisierungswerte für Variablen)
        - BSS-Region (Variablendeklarationen)
        - Heap-Region
        - Stack-Region (Heap und Stack wachsen aufeinander zu)
        - Argument-Region (Argumente für das Programm)
        - Environment-Region (z.B. PATH)


Virtueller Speicher
**************************************************************

Virtualisierung des Speichers:
- Jeder Prozess hat virtuell gesamten Adressraum zur Verfügung, d.h. kann ihn beliebig belegen
- Jeder Prozess ist gegen Fehlzugriffe aller anderen Prozesse geschützt
- Ensteht Knappheit im Hauptspeicher werden Inhalte auf Hintergrundspeicher ausgelagert
- Virtual Memory Manager übernimmt die Virtualisierung


Adressumsetzung (address translation):
- Mapping vom virtuellen Speicher zum Realen ("wenn Prozess X auf Adresse Y zugreift, dann schaue in Adresse Z")
- Sehr schwierige Aufgabe
- Aufgabe wird von Adressumsetzungshardware übernommen (MMU). Diese sitzt zwischen CPU und Speicherbus
- MMU ist Hardware
- Umsetzungstabellen:
    - Eine pro Prozess
    - Legt fest, wie virtuelle Speicherregionen des Prozesses in physischen Speicher umgesetzt werden
    - MMU muss über Prozessumschaltung informiert werden, damit sie weiss, welche Umsetzungstabelle sie verwenden muss
    - Problem: Jeder Speicherzugriff führt zu zwei Speicherzugriffen (1. Rausfinden, wo Info ist (Umsetzungstabelle); 2. Auf Datum zugreifen)
    - Lösung: Pufferung von Tabelleninhalten in der MMU
    - Vorteile Umsetzungstabelle:
        1. Prozesse können frei über ihren privaten virtuellen Adressraum verfügen
        2. Prozesse müssen keine Rücksicht nehmen auf physisch effektiv vorhandene Adressbereiche 
        3. Jeder Prozess "sieht" nur seine eigenen Speicherinhalte -> Speicherschutz
- Umsetzungsregel: Regel, die festlegt, wie man von logischer (virtueller) Adresse zu physischer Adresse kommt
- 3 Möglichkeiten zur Adressumsetzung:
    1. Segmentbasierte Umsetzung:
        - Phyischer Speicher wird segmentweise zur Verfügung gestellt (Segmente haben unterschiedliche Grössen)
        - Prozess kann mehrere Segmente besetzen
        - Nachteil: Externe Fragmentierung
        - Umsetzungsregel:
            - AP = AL + AS
            - AP: physische Adresse
            - AL: logische Adresse
            - AS: Segmentstartadresse
        - Segmentdeskriptorentabelle:
            - Enthält pro Segmentnummer einen Eintrag
            - Eintrag enthält Segmentstartadresse und Segmentlänge
            - Falls AL (logische Adresse) grösser als Segmentlänge -> Fehler
    2. Seitenbasierte Umsetzung:
        - Virtueller Speicherbereich wird in Blöcke fester Grösse aufgeteilt -> Seiten (pages)
        - Blockgrösse: 2^k Byte (normalerweise 4 KB) -> Vereinfachte Realisierung der MMU
        - Physischer Speicherbereich wird in Blöcke gleicher Grösse aufgeteilt -> Seitenrahmen (page frames)
        - Vorteil: Keine externe Fragmentierung
        - Adressumsetzung: Von page auf page frame schliessen
        - Eine Umsetzungstabelle pro Prozess
        - Inhalt von Eintrag in Umsetzungstabelle:
            - Seitenrahmennummer
            - Gültigkeitsbit
            - Schreibschutzbit
        - Umsetzungsregel:
            - AP = AL + v * S
            - AP: physische Adresse
            - AL: logische Adresse
            - v: Verschiebungsfaktor (vorzeichenbehafteter Ganzzahlwert, ergibt sich aus Umsetzungstabelle)
            - S: Seitengröße (2^k, k ist konstant)
        - Welcher Eintrag in Umsetzungstabelle gilt für bestimmte virtuelle Adresse?
            -> Seiten werden beginnent bei Null lückenlos aufsteigend durchnummeriert
            - PV = AL Div S
            - PV: Seitennummer
            - AL: logische Adresse
            - S: Seitengröße (2^k, k ist konstant)
            - Div: Ganzzahldivision (Hier einfach durch Bitshifting, weil k immer Zweierpotenz -> k niederwertigste Bit wegwerfen)
        - Bestimmung der Seitenrahmennummer:
            - PP = PV + v
            - PP: Seitenrahmennummer (positiver Ganzzahlwert inklusive 0)
            - PV: Seitennummer (positiver Ganzzahlwert inklusive 0)
            - v: Verschiebungsfaktor (vorzeichenbehafteter Ganzzahlwert)
        - Bestimmung der Relativadresse (page offset):
            - AR = AL Mod S
            - AR: Relativadresse (relativ zu Seitenbeginn)
            - AL: logische Adresse
            - S: Seitengröße (2^k, k ist konstant)
            - Mod: Modulo-Division (Hier einfach durch Bitshifting, weil k immer Zweierpotenz -> k niederwertigste Bit sind der Modulus)
        - Vorgehen Umsetzung:
            1. Seitennummer PV aus AL extrahieren -> Index in Umsetzungstabelle
            2. Seitenrahmennummer PP wird aus Umsetzungstabelle entnommen
            3. PP wird mit S multipliziert -> Seitenrahmenstartadresse ist bekannt
            4. Relativadresse AR zu Seitenrahmenstartadresse addieren
        - Platzbedarf pro Umsetzungstabelle (d.h. pro Prozess):
            - Platzbedarf in Byte = 2^m * t / s
            - m: Anzahl Adressbit der virtuellen Adresse
            - t: Größe eines Seitendeskriptors in Anzahl an Byte
            - s: Seitengröße in Anzahl Byte (Annahme: pro Adresse werde 1 Byte an Speicherplatz adressiert)
            - Beispiel:
                -> m = 32, s = 4 KB (= 2^12 Byte), t = 4 Byte
                -> Platzbedarf = 2^22 Byte = 4 MB (4 * [2^32]/[2^12] = 4 * [2^20] = 4 * 1024 KB = 4 MB)
        -> Umsetzungstabellen sind also speicherplatzmässig sehr teuer
        - Lösung: Mehrstufige Tabellen (kaskadierende Tabellen)
            - Erster Tabellenzugriff liefert nicht direkt Seitenrahmennummer, sondern Startadresse einer zweiten Tabelle
            - Es gibt auch dreistufige Lösungen
            - Vorteil: Wenn virtueller Adressraum nur spärlich belegt ist (in der Praxis oft der Fall)
            - Wichtig: Gut funktionierender TLB, da sonst drei (bzw. vier) Speicherzugriffe pro CPU-Zugriff
    3. Segment- und seitenbasierte Adressumsetzung: Mischform
